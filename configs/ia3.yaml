### Model Arguments
model_name_or_path: "Qwen/Qwen3-0.6B"
cache_dir: "./model_cache"
model_max_length: 2048
trust_remote_code: true
torch_dtype: "bfloat16"
device_map: "auto"
attn_implementation: "flash_attention_2"

### Data Arguments
train_file: "./dataset/processed/train.parquet"
validation_file: "./dataset/processed/validation.parquet"
preprocessing_num_workers: 1
max_train_samples: null
max_eval_samples: null
packing: true

use_lora: false
use_qlora: false

use_ia3: true
ia3_target_modules:
  - "k_proj"
  - "v_proj"
  - "down_proj"
ia3_feedforward_modules:
  - "down_proj"
ia3_modules_to_save: null

### Training Arguments
output_dir: "./outputs/ia3_finetune"
num_train_epochs: 5.0
per_device_train_batch_size: 8
per_device_eval_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 0.005 # Slightly increased for larger batch size
max_grad_norm: 1.0
warmup_ratio: 0.03
lr_scheduler_type: "cosine"
logging_steps: 1
save_strategy: "steps"
save_steps: 50
eval_strategy: "steps"
eval_steps: 50
bf16: true
fp16: false
tf32: true
optim: "adamw_torch_fused"
report_to:
  - "wandb"
dataloader_pin_memory: true
dataloader_num_workers: 1
